{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Understanding the Amazon from Space**","metadata":{}},{"cell_type":"markdown","source":"# **Image Classification Using The Neural Network**","metadata":{}},{"cell_type":"markdown","source":"Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively.\n\nIn this notebook, I have used the dataset with jpg files to train the machine learning model for image classification. The Convolutional Neural Network (CNN) is used in order to classify the satellite images as it automatically detects the important features of the image without any supervision.\n\nHere, I have used various Python libraries such as numpy, pandas, tensorflow, keras, etc to acquire the results we expecting from the dataset.","metadata":{}},{"cell_type":"markdown","source":"**Importing the libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport gc\nimport numpy as np\n\nfrom keras import backend as K\nfrom sklearn.metrics import fbeta_score\nfrom keras.layers import Conv2D, Dense, LSTM, Flatten, MaxPooling2D, BatchNormalization, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.metrics import fbeta_score\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\n\nimport cv2\nfrom PIL import Image\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential \n\nimport seaborn as sns\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, History","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:09:29.860128Z","iopub.execute_input":"2023-01-17T18:09:29.860834Z","iopub.status.idle":"2023-01-17T18:09:36.015514Z","shell.execute_reply.started":"2023-01-17T18:09:29.860707Z","shell.execute_reply":"2023-01-17T18:09:36.006075Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Loading the datasets**","metadata":{}},{"cell_type":"code","source":"#reading the labels\n\ntrain_label = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\nsam_sub = pd.read_csv('../input/planets-dataset/planet/planet/sample_submission.csv')\ntrain_label.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:09:36.017562Z","iopub.execute_input":"2023-01-17T18:09:36.018250Z","iopub.status.idle":"2023-01-17T18:09:36.166568Z","shell.execute_reply.started":"2023-01-17T18:09:36.018213Z","shell.execute_reply":"2023-01-17T18:09:36.165609Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"  image_name                                       tags\n0    train_0                               haze primary\n1    train_1            agriculture clear primary water\n2    train_2                              clear primary\n3    train_3                              clear primary\n4    train_4  agriculture clear habitation primary road","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>haze primary</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>agriculture clear primary water</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>clear primary</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>clear primary</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>agriculture clear habitation primary road</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#onehot=OneHotEncoder()\n\nencoder = LabelEncoder()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:09:36.167830Z","iopub.execute_input":"2023-01-17T18:09:36.168793Z","iopub.status.idle":"2023-01-17T18:09:36.173290Z","shell.execute_reply.started":"2023-01-17T18:09:36.168757Z","shell.execute_reply":"2023-01-17T18:09:36.172148Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#encoder.fit(train_label['tags'])\n\nlabel_maps = pd.DataFrame()\nlabel_maps['tags'] = ['agriculture', 'artisinal_mine', 'bare_ground','blooming','blow_down','clear','cloudy','conventional_mine','cultivation','habitation','haze', 'partly_cloudy','primary','road','selective_logging','slash_burn','water']\nlabel_maps['map'] = encoder.fit_transform(label_maps['tags'])\nlabel_maps","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:09:36.176126Z","iopub.execute_input":"2023-01-17T18:09:36.177252Z","iopub.status.idle":"2023-01-17T18:09:36.196813Z","shell.execute_reply.started":"2023-01-17T18:09:36.177216Z","shell.execute_reply":"2023-01-17T18:09:36.195777Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 tags  map\n0         agriculture    0\n1      artisinal_mine    1\n2         bare_ground    2\n3            blooming    3\n4           blow_down    4\n5               clear    5\n6              cloudy    6\n7   conventional_mine    7\n8         cultivation    8\n9          habitation    9\n10               haze   10\n11      partly_cloudy   11\n12            primary   12\n13               road   13\n14  selective_logging   14\n15         slash_burn   15\n16              water   16","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tags</th>\n      <th>map</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>agriculture</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>artisinal_mine</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bare_ground</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>blooming</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>blow_down</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>clear</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>cloudy</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>conventional_mine</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>cultivation</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>habitation</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>haze</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>partly_cloudy</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>primary</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>road</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>selective_logging</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>slash_burn</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>water</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#defining a dict of encoded labels\n\nlabel_map = {'agriculture': 0,\n 'artisinal_mine': 1,\n 'bare_ground': 2,\n 'blooming': 3,\n 'blow_down': 4,\n 'clear': 5,\n 'cloudy': 6,\n 'conventional_mine': 7,\n 'cultivation': 8,\n 'habitation': 9,\n 'haze': 10,\n 'partly_cloudy': 11,\n 'primary': 12,\n 'road': 13,\n 'selective_logging': 14,\n 'slash_burn': 15,\n 'water': 16}\n","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:09:36.197944Z","iopub.execute_input":"2023-01-17T18:09:36.198264Z","iopub.status.idle":"2023-01-17T18:09:36.204103Z","shell.execute_reply.started":"2023-01-17T18:09:36.198231Z","shell.execute_reply":"2023-01-17T18:09:36.203095Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#loading the traing_images\n\nX = []\nY = []\ntrain_label = shuffle(train_label,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:09:36.205752Z","iopub.execute_input":"2023-01-17T18:09:36.206453Z","iopub.status.idle":"2023-01-17T18:09:36.219697Z","shell.execute_reply.started":"2023-01-17T18:09:36.206418Z","shell.execute_reply":"2023-01-17T18:09:36.219065Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for image_name, tags in tqdm(train_label.values, miniters=400):\n    arr = cv2.imread('../input/planets-dataset/planet/planet/train-jpg/{}.jpg'.format(image_name), cv2.IMREAD_UNCHANGED)\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n      targets[label_map[t]] = 1 \n    arr = cv2.resize(arr, (64, 64))\n    X.append(arr)\n    Y.append(targets)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:09:36.221118Z","iopub.execute_input":"2023-01-17T18:09:36.221847Z","iopub.status.idle":"2023-01-17T18:14:17.491583Z","shell.execute_reply.started":"2023-01-17T18:09:36.221812Z","shell.execute_reply":"2023-01-17T18:14:17.489966Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 40479/40479 [04:41<00:00, 143.92it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"X = np.array(X, np.float16)/255.0","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:14:17.493104Z","iopub.execute_input":"2023-01-17T18:14:17.493493Z","iopub.status.idle":"2023-01-17T18:14:26.007439Z","shell.execute_reply.started":"2023-01-17T18:14:17.493443Z","shell.execute_reply":"2023-01-17T18:14:26.006437Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#splitting\n\nX = np.array(X)\nY = np.array(Y)\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, shuffle = True, random_state = 1)\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:14:26.013856Z","iopub.execute_input":"2023-01-17T18:14:26.014162Z","iopub.status.idle":"2023-01-17T18:14:26.700539Z","shell.execute_reply.started":"2023-01-17T18:14:26.014135Z","shell.execute_reply":"2023-01-17T18:14:26.699422Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(32383, 64, 64, 3) (32383, 17) (8096, 64, 64, 3) (8096, 17)\n","output_type":"stream"}]},{"cell_type":"code","source":"del X, Y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:14:26.704572Z","iopub.execute_input":"2023-01-17T18:14:26.704881Z","iopub.status.idle":"2023-01-17T18:14:26.870240Z","shell.execute_reply.started":"2023-01-17T18:14:26.704854Z","shell.execute_reply":"2023-01-17T18:14:26.869144Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"69"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:14:26.871691Z","iopub.execute_input":"2023-01-17T18:14:26.872642Z","iopub.status.idle":"2023-01-17T18:14:27.021170Z","shell.execute_reply.started":"2023-01-17T18:14:26.872602Z","shell.execute_reply":"2023-01-17T18:14:27.019483Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"code","source":"def fbeta(y_true, y_pred, threshold_shift=0):\n    beta = 2\n\n    # just in case of hipster activation at the final layer\n    y_pred = K.clip(y_pred, 0, 1)\n\n    # shifting the prediction threshold from .5 if needed\n    y_pred_bin = K.round(y_pred + threshold_shift)\n\n    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n\n    beta_squared = beta ** 2\n    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:14:27.023930Z","iopub.execute_input":"2023-01-17T18:14:27.025599Z","iopub.status.idle":"2023-01-17T18:14:27.033418Z","shell.execute_reply.started":"2023-01-17T18:14:27.025570Z","shell.execute_reply":"2023-01-17T18:14:27.032381Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:14:27.034978Z","iopub.execute_input":"2023-01-17T18:14:27.035353Z","iopub.status.idle":"2023-01-17T18:14:27.043123Z","shell.execute_reply.started":"2023-01-17T18:14:27.035320Z","shell.execute_reply":"2023-01-17T18:14:27.042142Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Define the model**","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(Conv2D(64, 5, 2, activation = \"relu\", input_shape = (64, 64, 3)))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(128, 5, 2, activation = \"relu\"))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(Dense(17, activation = \"sigmoid\"))\nmodel.compile(loss = \"binary_crossentropy\", optimizer = Adam(), metrics = [fbeta])\nmodel.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 45, batch_size = 64)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T18:14:27.045200Z","iopub.execute_input":"2023-01-17T18:14:27.045456Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2023-01-17 18:14:27.135917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:27.222869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:27.223646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:27.226129: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-17 18:14:27.226459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:27.227201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:27.227859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:29.047732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:29.048639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:29.049356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-17 18:14:29.049956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with tpu_strategy.scope():\n\ntest_loss, test_accuracy = model.evaluate(x_val, y_val)\nprint('Test loss: {}'.format(test_loss))\nprint('Test accuracy: {}'.format(test_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x_train, y_train, x_val, y_val\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dividing my test_labels into two part for test-jpg and test-jpg-additional\ntest = sam_sub[0 : 40669]\nfiles = sam_sub[40669 : ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with tpu_strategy.scope():  \n\ntest_img = []\n\nfor image_name, tags in tqdm(test.values, miniters=1000):\n    arr = cv2.imread('../input/planets-dataset/planet/planet/test-jpg/{}.jpg'.format(image_name))\n    test_img.append(cv2.resize(arr, (64, 64)))\n\nfor image_name, tags in tqdm(files.values, miniters=1000):\n    arr = cv2.imread('../input/planets-dataset/test-jpg-additional/test-jpg-additional/{}.jpg'.format(image_name))\n    test_img.append(cv2.resize(arr, (64, 64)))\n\ntest_img = np.array(test_img, np.float16)/255.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with tpu_strategy.scope():\n\nyres = []\npredictions = model.predict(test_img, batch_size = 64, verbose = 2)\nyres.append(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting my encoded labels back to it original form\n\nsub = np.array(yres[0])\nfor i in range (1, len(yres)):\n    sub += np.array(yres[i])\nsub = pd.DataFrame(sub, columns = label_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor i in tqdm(range(sub.shape[0]), miniters=1000):\n    a = sub.loc[[i]]\n    a = a.apply(lambda x: x > 0.2, axis=1)\n    a = a.transpose()\n    a= a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))\n\nsam_sub['tags'] = preds\nsam_sub.to_csv('subm.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finally, I have saved the final result into the sample submission file (csv format).**","metadata":{}}]}